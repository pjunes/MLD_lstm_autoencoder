{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "11.16 Data Checking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1120 정상 비정상 데이터 같이 있는 데이터로 훈련 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5JfKAM45tNT7"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pylab import rcParams\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import optimizers, Sequential\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "\n",
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
        "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
        "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(7)\n",
        "tf.random.set_seed(11)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "SEED = 123   # used to help randomly select the data points\n",
        "DATA_SPLIT_PCT = 0.2\n",
        "\n",
        "rcParams['figure.figsize'] = 8, 6\n",
        "LABELS = [\"Normal\",\"Break\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "# lab pc file dir\n",
        "path_1 = \"C:/Users/sue33/Desktop/pjuns/DB/EM/Training/vibration/vibration/2.2kW/L-DEF-01/정상/\"\n",
        "path_2 = \"C:/Users/sue33/Desktop/pjuns/DB/EM/Training/vibration/vibration/2.2kW/L-DSF-01/축정렬불량/\"\n",
        "\n",
        "file_list_1 = os.listdir(path_1)[:]\n",
        "# file_list_2 = os.listdir(path_2)[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_list = file_list_1\n",
        "# file_list = file_list_1 + file_list_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(len(file_list_1))\n",
        "# print(len(file_list_2))\n",
        "# print(len(file_list_1) + len(file_list_2))\n",
        "# print(len(file_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QLkcPUukpH6C"
      },
      "outputs": [],
      "source": [
        "raw_data = []\n",
        "\n",
        "for i, file_name in enumerate(file_list):\n",
        "    if i < 15290:\n",
        "        f = open(path_1+file_name, 'r', encoding='utf-8')\n",
        "    else:\n",
        "        f = open(path_2+file_name, 'r', encoding='utf-8')\n",
        "\n",
        "    # rdr : file to list\n",
        "    rdr = list(csv.reader(f))\n",
        "\n",
        "    label = int(rdr[3][1]) # y\n",
        "    period = rdr[5][1]\n",
        "    rate = rdr[6][1]\n",
        "    RMS_value = rdr[7][1]\n",
        "    length = rdr[8][1]\n",
        "\n",
        "    value = np.array(list(rdr[9:]))[:, 1:2] # X\n",
        "    # value = np.array(value)[:, 1:2]\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    data = [value, label, period, rate, RMS_value, length]\n",
        "    raw_data.append(data)\n",
        "    \n",
        "    # df.append(pd.DataFrame(data, columns=column_name))\n",
        "    \n",
        "    # if i == 10:\n",
        "    #     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af_bCANjwTKD",
        "outputId": "706a55c7-ba96-4d09-a0b0-2b86f838cb7d"
      },
      "outputs": [],
      "source": [
        "# print(np.array(raw_data)[:, 1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "column_name = ['y', 'x']\n",
        "temporalize_df = []\n",
        "\n",
        "for d in raw_data:\n",
        "    if d[1] == 0:\n",
        "        temp = np.concatenate((np.zeros((12000, 1)), d[0]), axis=1)\n",
        "    else:\n",
        "        temp = np.concatenate((np.ones((12000, 1)), d[0]), axis=1)\n",
        "        \n",
        "    temporalize_df.append(pd.DataFrame(temp, columns=column_name).astype(np.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(temporalize_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VIXbHzkP0f02"
      },
      "outputs": [],
      "source": [
        "def temporalize(X, y, lookback):\n",
        "    output_X = []\n",
        "    output_y = []\n",
        "    for i in range(0, len(X)-lookback-1, lookback//2):\n",
        "        t = []\n",
        "        for j in range(1,lookback+1):\n",
        "            # Gather past records upto the lookback period\n",
        "            t.append(X[[(i+j+1)], :])\n",
        "        output_X.append(t)\n",
        "        output_y.append(y[i+lookback+1])\n",
        "    return output_X, output_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# n_features = input_X.shape[1]\n",
        "n_features = 1\n",
        "lookback = 8\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for d in temporalize_df:\n",
        "    input_X = d.loc[:, d.columns != 'y'].values\n",
        "    input_y = d['y'].values\n",
        "\n",
        "    nX, ny = temporalize(X = input_X, y = input_y, lookback = lookback)\n",
        "    X.extend(nX)\n",
        "    y.extend(ny)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ### temporalize by file ###\n",
        "# for d in temporalize_df:\n",
        "#     input_X = d.loc[:, d.columns != 'y'].values\n",
        "#     input_y = d['y'].values\n",
        "\n",
        "#     nX, ny = temporalize(X = input_X, y = input_y, lookback = lookback)\n",
        "#     X_byfile.append(nX)\n",
        "#     y_byfile.append(ny)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ### all for test ###\n",
        "# X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=1, random_state=SEED)\n",
        "# X_test = X_test.reshape(X_test.shape[0], lookback, n_features)\n",
        "# X_test_scaled = scale(X_test, scaler).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test_x_predictions = lstm_autoencoder.predict(X_test_scaled)\n",
        "# mse = np.mean(np.power(flatten(X_test_scaled) - flatten(test_x_predictions), 2), axis=1)\n",
        "\n",
        "# error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
        "#                         'True_class': y_test.tolist()})\n",
        "\n",
        "# threshold_fixed = 0.3\n",
        "# groups = error_df.groupby('True_class')\n",
        "# fig, ax = plt.subplots()\n",
        "\n",
        "# for name, group in groups:\n",
        "#     ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
        "#             label= \"Break\" if name == 1 else \"Normal\")\n",
        "# ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
        "# ax.legend()\n",
        "# plt.title(\"Reconstruction error for different classes\")\n",
        "# plt.ylabel(\"Reconstruction error\")\n",
        "# plt.xlabel(\"Data point index\")\n",
        "# plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]\n",
        "# conf_matrix = confusion_matrix(error_df.True_class, pred_y)\n",
        "\n",
        "# plt.figure(figsize=(6, 6))\n",
        "# sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
        "# plt.title(\"Confusion matrix\")\n",
        "# plt.ylabel('True class')\n",
        "# plt.xlabel('Predicted class')\n",
        "# plt.show()\n",
        "# false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.True_class, error_df.Reconstruction_error)\n",
        "# roc_auc = auc(false_pos_rate, true_pos_rate,)\n",
        "\n",
        "# plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\n",
        "# plt.plot([0,1],[0,1], linewidth=5)\n",
        "\n",
        "# plt.xlim([-0.01, 1])\n",
        "# plt.ylim([0, 1.01])\n",
        "# plt.legend(loc='lower right')\n",
        "# plt.title('Receiver operating characteristic curve (ROC)')\n",
        "# plt.ylabel('True Positive Rate')\n",
        "# plt.xlabel('False Positive Rate')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M6SUp6Zb1-ti"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=DATA_SPLIT_PCT, random_state=SEED)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=DATA_SPLIT_PCT, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LzHwx3-M2EP3"
      },
      "outputs": [],
      "source": [
        "# X_train_y0 = X_train[y_train==0.]\n",
        "X_train_y0 = X_train[y_train==0]\n",
        "X_train_y1 = X_train[y_train==1]\n",
        "\n",
        "X_valid_y0 = X_valid[y_valid==0]\n",
        "X_valid_y1 = X_valid[y_valid==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZXLPmSX2Feq",
        "outputId": "bc5d5363-1b72-4e47-dad5-fd39d93c3470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29337228\n",
            "0\n",
            "7334308\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train_y0))\n",
        "print(len(X_train_y1))\n",
        "print(len(X_valid_y0))\n",
        "print(len(X_valid_y1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YSRo1zZ52G6M"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], lookback, n_features)\n",
        "X_train_y0 = X_train_y0.reshape(X_train_y0.shape[0], lookback, n_features)\n",
        "X_train_y1 = X_train_y1.reshape(X_train_y1.shape[0], lookback, n_features)\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], lookback, n_features)\n",
        "\n",
        "X_valid = X_valid.reshape(X_valid.shape[0], lookback, n_features)\n",
        "X_valid_y0 = X_valid_y0.reshape(X_valid_y0.shape[0], lookback, n_features)\n",
        "X_valid_y1 = X_valid_y1.reshape(X_valid_y1.shape[0], lookback, n_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1P06eZEc2IuN"
      },
      "outputs": [],
      "source": [
        "def flatten(X):\n",
        "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
        "    for i in range(X.shape[0]):\n",
        "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
        "    return(flattened_X)\n",
        "\n",
        "def scale(X, scaler):\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i, :, :] = scaler.transform(X[i, :, :])\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bwwX9O-Z2Jcs"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler().fit(flatten(X_train_y0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xh0R84Xa5-q7"
      },
      "outputs": [],
      "source": [
        "X_train_y0_scaled = scale(X_train_y0, scaler).astype(np.float32)\n",
        "X_train_y1_scaled = scale(X_train_y1, scaler).astype(np.float32)\n",
        "X_train_scaled = scale(X_train, scaler).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh41GNWm6AF5",
        "outputId": "16fa09e5-6240-4931-a241-9f66cf1572af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "colwise mean [-0.]\n",
            "colwise variance [1.]\n"
          ]
        }
      ],
      "source": [
        "a = flatten(X_train_y0_scaled)\n",
        "print('colwise mean', np.mean(a, axis=0).round(6))\n",
        "print('colwise variance', np.var(a, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "d20vzMRR6Bmz"
      },
      "outputs": [],
      "source": [
        "X_valid_scaled = scale(X_valid, scaler).astype(np.float32)\n",
        "X_valid_y0_scaled = scale(X_valid_y0, scaler).astype(np.float32)\n",
        "X_test_scaled = scale(X_test, scaler).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Z_-yMTeW6C_j"
      },
      "outputs": [],
      "source": [
        "timesteps =  X_train_y0_scaled.shape[1] # equal to the lookback\n",
        "n_features =  X_train_y0_scaled.shape[2] # 59\n",
        "\n",
        "epochs = 200\n",
        "batch = 512\n",
        "# lr = 0.0003\n",
        "lr = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'tensorflow_backend'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[1;32m----> 3\u001b[0m tf\u001b[39m.\u001b[39;49mtensorflow_backend\u001b[39m.\u001b[39m_get_available_gpus()\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'tensorflow_backend'"
          ]
        }
      ],
      "source": [
        "# from keras import backend\n",
        "\n",
        "# tf.tensorflow_backend._get_available_gpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHCl6Zvw6EJV",
        "outputId": "b31d2649-2815-4233-9f5a-fe7b0dceee8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 8, 32)             4352      \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 16)                3136      \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVect  (None, 8, 16)            0         \n",
            " or)                                                             \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 8, 16)             2112      \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 8, 32)             6272      \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 8, 1)             33        \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,905\n",
            "Trainable params: 15,905\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "lstm_autoencoder = Sequential()\n",
        "# Encoder\n",
        "lstm_autoencoder.add(LSTM(32, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
        "lstm_autoencoder.add(LSTM(16, activation='relu', return_sequences=False))\n",
        "lstm_autoencoder.add(RepeatVector(timesteps))\n",
        "# Decoder\n",
        "lstm_autoencoder.add(LSTM(16, activation='relu', return_sequences=True))\n",
        "lstm_autoencoder.add(LSTM(32, activation='relu', return_sequences=True))\n",
        "lstm_autoencoder.add(TimeDistributed(Dense(n_features)))\n",
        "\n",
        "lstm_autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFxPP6OE6GJs",
        "outputId": "1a9f96d2-66c6-4352-d4a5-c1e8cdfd79d3"
      },
      "outputs": [],
      "source": [
        "adam = optimizers.Adam(lr)\n",
        "lstm_autoencoder.compile(loss='mse', optimizer=adam)\n",
        "\n",
        "cp = ModelCheckpoint(filepath=\"lstm_autoencoder_classifier.h5\",\n",
        "                               save_best_only=True,\n",
        "                               verbose=0)\n",
        "\n",
        "tb = TensorBoard(log_dir='./logs',\n",
        "                histogram_freq=0,\n",
        "                write_graph=True,\n",
        "                write_images=True)\n",
        "\n",
        "# es = EarlyStopping(patience=20)\n",
        "# mc = ModelCheckpoint(\"your_path/file_name.h5\", save_best_only=True)\n",
        "# rlr = ReduceLROnPlateau(factor=0.3, patience=5)\n",
        "# csvlogger = CSVLogger(\"your_path/file_name.log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "28650/28650 - 765s - loss: 0.1094 - val_loss: 0.0183 - 765s/epoch - 27ms/step\n",
            "Epoch 2/80\n",
            "28650/28650 - 753s - loss: 0.0074 - val_loss: 0.0024 - 753s/epoch - 26ms/step\n",
            "Epoch 3/80\n",
            "28650/28650 - 757s - loss: 0.0043 - val_loss: 0.0014 - 757s/epoch - 26ms/step\n",
            "Epoch 4/80\n",
            "28650/28650 - 748s - loss: 0.0019 - val_loss: 0.0020 - 748s/epoch - 26ms/step\n",
            "Epoch 5/80\n",
            "28650/28650 - 745s - loss: 0.0012 - val_loss: 0.0022 - 745s/epoch - 26ms/step\n",
            "Epoch 6/80\n",
            "28650/28650 - 740s - loss: 0.0010 - val_loss: 0.0010 - 740s/epoch - 26ms/step\n",
            "Epoch 7/80\n",
            "28650/28650 - 742s - loss: 8.5052e-04 - val_loss: 5.3103e-04 - 742s/epoch - 26ms/step\n",
            "Epoch 8/80\n",
            "28650/28650 - 743s - loss: 7.4319e-04 - val_loss: 5.7618e-04 - 743s/epoch - 26ms/step\n",
            "Epoch 9/80\n",
            "28650/28650 - 741s - loss: 6.7144e-04 - val_loss: 4.1500e-04 - 741s/epoch - 26ms/step\n",
            "Epoch 10/80\n",
            "28650/28650 - 739s - loss: 5.9544e-04 - val_loss: 3.8998e-04 - 739s/epoch - 26ms/step\n",
            "Epoch 11/80\n",
            "28650/28650 - 740s - loss: 5.3474e-04 - val_loss: 3.8235e-04 - 740s/epoch - 26ms/step\n",
            "Epoch 12/80\n",
            "28650/28650 - 738s - loss: 4.9021e-04 - val_loss: 5.7276e-04 - 738s/epoch - 26ms/step\n",
            "Epoch 13/80\n",
            "28650/28650 - 740s - loss: 4.4628e-04 - val_loss: 3.2533e-04 - 740s/epoch - 26ms/step\n",
            "Epoch 14/80\n",
            "28650/28650 - 741s - loss: 5.1382e-04 - val_loss: 5.5144e-04 - 741s/epoch - 26ms/step\n",
            "Epoch 15/80\n",
            "28650/28650 - 742s - loss: 3.8563e-04 - val_loss: 0.0017 - 742s/epoch - 26ms/step\n",
            "Epoch 16/80\n",
            "28650/28650 - 746s - loss: 3.5756e-04 - val_loss: 2.6934e-04 - 746s/epoch - 26ms/step\n",
            "Epoch 17/80\n",
            "28650/28650 - 745s - loss: 3.2793e-04 - val_loss: 2.7120e-04 - 745s/epoch - 26ms/step\n",
            "Epoch 18/80\n",
            "28650/28650 - 744s - loss: 3.0706e-04 - val_loss: 2.4631e-04 - 744s/epoch - 26ms/step\n",
            "Epoch 19/80\n",
            "28650/28650 - 746s - loss: 2.8696e-04 - val_loss: 3.7409e-04 - 746s/epoch - 26ms/step\n",
            "Epoch 20/80\n",
            "28650/28650 - 747s - loss: 2.7425e-04 - val_loss: 2.7490e-04 - 747s/epoch - 26ms/step\n",
            "Epoch 21/80\n",
            "28650/28650 - 748s - loss: 2.5440e-04 - val_loss: 3.4478e-04 - 748s/epoch - 26ms/step\n",
            "Epoch 22/80\n",
            "28650/28650 - 745s - loss: 2.4066e-04 - val_loss: 2.9125e-04 - 745s/epoch - 26ms/step\n",
            "Epoch 23/80\n",
            "28650/28650 - 742s - loss: 2.2614e-04 - val_loss: 1.8401e-04 - 742s/epoch - 26ms/step\n",
            "Epoch 24/80\n",
            "28650/28650 - 743s - loss: 2.2379e-04 - val_loss: 2.6896e-04 - 743s/epoch - 26ms/step\n",
            "Epoch 25/80\n",
            "28650/28650 - 746s - loss: 2.0595e-04 - val_loss: 3.2614e-04 - 746s/epoch - 26ms/step\n",
            "Epoch 26/80\n",
            "28650/28650 - 744s - loss: 1.9476e-04 - val_loss: 1.6328e-04 - 744s/epoch - 26ms/step\n",
            "Epoch 27/80\n",
            "28650/28650 - 767s - loss: 1.8610e-04 - val_loss: 1.2805e-04 - 767s/epoch - 27ms/step\n",
            "Epoch 28/80\n"
          ]
        }
      ],
      "source": [
        "lstm_autoencoder_history = lstm_autoencoder.fit(X_train_y0_scaled, X_train_y0_scaled, \n",
        "                                                epochs=epochs, \n",
        "                                                batch_size=batch, \n",
        "                                                validation_data=(X_valid_y0_scaled, X_valid_y0_scaled),\n",
        "                                                # callbacks=[es, mc, rlr, csvlogger],\n",
        "                                                verbose=2).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# adam = optimizers.Adam(lr)\n",
        "# lstm_autoencoder.compile(loss='mse', optimizer=adam)\n",
        "\n",
        "# cp = ModelCheckpoint(filepath=\"lstm_autoencoder_classifier.h5\",\n",
        "#                                save_best_only=True,\n",
        "#                                verbose=0)\n",
        "\n",
        "# tb = TensorBoard(log_dir='./logs',\n",
        "#                 histogram_freq=0,\n",
        "#                 write_graph=True,\n",
        "#                 write_images=True)\n",
        "\n",
        "# lstm_autoencoder_history = lstm_autoencoder.fit(X_train_y0_scaled, X_train_y0_scaled,\n",
        "#                                                 epochs=epochs,\n",
        "#                                                 batch_size=batch,\n",
        "#                                                 validation_data=(X_valid_y0_scaled, X_valid_y0_scaled),\n",
        "#                                                 verbose=2).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n"
          ]
        },
        {
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xc1 in position 63: invalid start byte",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lstm_autoencoder_history \u001b[39m=\u001b[39m lstm_autoencoder\u001b[39m.\u001b[39;49mfit(X_train_y0_scaled, X_train_y0_scaled, \n\u001b[0;32m      2\u001b[0m                                                 epochs\u001b[39m=\u001b[39;49mepochs, \n\u001b[0;32m      3\u001b[0m                                                 batch_size\u001b[39m=\u001b[39;49mbatch, \n\u001b[0;32m      4\u001b[0m                                                 validation_data\u001b[39m=\u001b[39;49m(X_valid_y0_scaled, X_valid_y0_scaled),\n\u001b[0;32m      5\u001b[0m                                                 callbacks\u001b[39m=\u001b[39;49m[es, mc, rlr, csvlogger],\n\u001b[0;32m      6\u001b[0m                                                 verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39mhistory\n",
            "File \u001b[1;32mc:\\Users\\sue33\\Desktop\\pjuns\\MLD\\MLD_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\csv.py:143\u001b[0m, in \u001b[0;36mDictWriter.writeheader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwriteheader\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    142\u001b[0m     header \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfieldnames, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfieldnames))\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriterow(header)\n",
            "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\csv.py:154\u001b[0m, in \u001b[0;36mDictWriter.writerow\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwriterow\u001b[39m(\u001b[39mself\u001b[39m, rowdict):\n\u001b[1;32m--> 154\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriter\u001b[39m.\u001b[39;49mwriterow(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dict_to_list(rowdict))\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc1 in position 63: invalid start byte"
          ]
        }
      ],
      "source": [
        "# lstm_autoencoder_history = lstm_autoencoder.fit(X_train_y0_scaled, X_train_y0_scaled, \n",
        "#                                                 epochs=epochs, \n",
        "#                                                 batch_size=batch, \n",
        "#                                                 validation_data=(X_valid_y0_scaled, X_valid_y0_scaled),\n",
        "#                                                 callbacks=[es, mc, rlr, csvlogger],\n",
        "#                                                 verbose=2).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "# from sklearn.externals import joblib\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saved_model = pickle.dumps(lstm_autoencoder)\n",
        "joblib.dump(lstm_autoencoder, \"1116_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lstm_autoencoder.save(\"1119_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.save(lstm_autoencoder, f'./lstm_autoencoder.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "zFwDyWCy-LxQ",
        "outputId": "d44b7c2c-efa4-43fe-bc7b-9a9337adf227"
      },
      "outputs": [],
      "source": [
        "plt.plot(lstm_autoencoder_history['loss'], linewidth=2, label='Train')\n",
        "plt.plot(lstm_autoencoder_history['val_loss'], linewidth=2, label='Valid')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "ZQOZZ7Fy-UgH",
        "outputId": "dceb7b78-d033-4d77-e4e7-9e8e2edf8d02"
      },
      "outputs": [],
      "source": [
        "train_x_predictions = lstm_autoencoder.predict(X_train_scaled)\n",
        "mse = np.mean(np.power(flatten(X_train_scaled) - flatten(train_x_predictions), 2), axis=1)\n",
        "\n",
        "# mse = np.mean(np.power(flatten(X_test_scaled) - flatten(test_x_predictions), 2), axis=1)\n",
        "\n",
        "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
        "                        'True_class': y_train.tolist()})\n",
        "\n",
        "# error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
        "#                         'True_class': y_test.tolist()})\n",
        "\n",
        "groups = error_df.groupby('True_class')\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for name, group in groups:\n",
        "    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
        "            label= \"Break\" if name == 1 else \"Normal\")\n",
        "ax.legend()\n",
        "plt.title(\"Reconstruction error for different classes\")\n",
        "plt.ylabel(\"Reconstruction error\")\n",
        "plt.xlabel(\"Data point index\")\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "3zduXdqM-Wx7",
        "outputId": "76a0ea01-accd-4028-eb99-3324410c028e"
      },
      "outputs": [],
      "source": [
        "valid_x_predictions = lstm_autoencoder.predict(X_valid_scaled)\n",
        "mse = np.mean(np.power(flatten(X_valid_scaled) - flatten(valid_x_predictions), 2), axis=1)\n",
        "\n",
        "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
        "                        'True_class': y_valid.tolist()})\n",
        "\n",
        "precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)\n",
        "plt.plot(threshold_rt, precision_rt[1:], label=\"Precision\",linewidth=5)\n",
        "plt.plot(threshold_rt, recall_rt[1:], label=\"Recall\",linewidth=5)\n",
        "plt.title('Precision and recall for different threshold values')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Precision/Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "r--FVA6--Yib",
        "outputId": "5642b59f-1728-4527-e80f-e722a25e6c15"
      },
      "outputs": [],
      "source": [
        "test_x_predictions = lstm_autoencoder.predict(X_test_scaled)\n",
        "mse = np.mean(np.power(flatten(X_test_scaled) - flatten(test_x_predictions), 2), axis=1)\n",
        "\n",
        "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
        "                        'True_class': y_test.tolist()})\n",
        "\n",
        "threshold_fixed = 0.3\n",
        "groups = error_df.groupby('True_class')\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for name, group in groups:\n",
        "    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
        "            label= \"Break\" if name == 1 else \"Normal\")\n",
        "ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
        "ax.legend()\n",
        "plt.title(\"Reconstruction error for different classes\")\n",
        "plt.ylabel(\"Reconstruction error\")\n",
        "plt.xlabel(\"Data point index\")\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHfns9jR-aRb"
      },
      "outputs": [],
      "source": [
        "pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "WSrLx6DP-buT",
        "outputId": "56da8584-b998-4eba-c2db-8ee2f13a05c0"
      },
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(error_df.True_class, pred_y)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.ylabel('True class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "DAs8HVKc-dWa",
        "outputId": "002b0928-f522-4346-b6a5-9a8fed8d801b"
      },
      "outputs": [],
      "source": [
        "false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.True_class, error_df.Reconstruction_error)\n",
        "roc_auc = auc(false_pos_rate, true_pos_rate,)\n",
        "\n",
        "plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\n",
        "plt.plot([0,1],[0,1], linewidth=5)\n",
        "\n",
        "plt.xlim([-0.01, 1])\n",
        "plt.ylim([0, 1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Receiver operating characteristic curve (ROC)')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('MLD_env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "dc2e52922f7cb6ebbd414ed8233ade6562b7b33014c367c18ca863d9959de4df"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
